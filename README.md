# CoreTelecoms Unified Customer Experience Data Platform


## üìã Table of Contents
- [Project Overview](#project-overview)
- [Business Problem](#business-problem)
- [Solution Architecture](#solution-architecture)
- [Technology Stack](#technology-stack)
- [Project Structure](#project-structure)
- [Data Sources](#data-sources)
- [Pipeline Architecture](#pipeline-architecture)
- [Setup Instructions](#setup-instructions)
- [Running the Pipeline](#running-the-pipeline)
- [Data Quality & Testing](#data-quality--testing)
- [Monitoring & Alerts](#monitoring--alerts)
- [CI/CD Pipeline](#cicd-pipeline)
- [Infrastructure as Code](#infrastructure-as-code)
- [Key Features](#key-features)
- [Future Enhancements](#future-enhancements)
- [Contributors](#contributors)

---

## üéØ Project Overview

CoreTelecoms, a leading US telecom company, faced a critical customer retention crisis due to fragmented complaint management systems. This project delivers a **production-grade, unified data platform** that consolidates customer complaints from multiple channels into a single source of truth for analytics, machine learning, and business insights.

![Alt text](docs/coretelecoms%20architecture.png)

### Key Achievements
- ‚úÖ Unified 5 disparate data sources into a single data warehouse
- ‚úÖ Automated daily ingestion of 100K+ complaint records
- ‚úÖ Reduced reporting time from days to minutes
- ‚úÖ Enabled real-time analytics and ML-driven insights
- ‚úÖ Implemented full CI/CD with infrastructure as code

---

## üíº Business Problem

### Challenges
CoreTelecoms struggled with:
- **Data Silos**: Complaints scattered across social media, call centers, and web forms
- **Manual Processes**: Reporting team manually compiled spreadsheets
- **Delayed Insights**: Reports took days to generate
- **Data Quality Issues**: Inconsistent formats, naming conventions, and missing values
- **Customer Churn**: Inability to identify and address complaint patterns quickly

### Impact
- Lost revenue due to customer churn
- Poor customer satisfaction scores
- Inefficient complaint resolution
- Limited visibility into operational metrics

---

## üèóÔ∏è Solution Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        DATA SOURCES                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   AWS S3     ‚îÇ  Google      ‚îÇ   AWS S3     ‚îÇ   PostgreSQL RDS   ‚îÇ
‚îÇ  Customers   ‚îÇ  Sheets      ‚îÇ  Call Logs   ‚îÇ   Web Forms        ‚îÇ
‚îÇ   (CSV)      ‚îÇ  Agents      ‚îÇ  (CSV)       ‚îÇ   (Tables)         ‚îÇ
‚îÇ              ‚îÇ              ‚îÇ  Social      ‚îÇ                    ‚îÇ
‚îÇ              ‚îÇ              ‚îÇ  Media       ‚îÇ                    ‚îÇ
‚îÇ              ‚îÇ              ‚îÇ  (JSON)      ‚îÇ                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ              ‚îÇ              ‚îÇ                ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ      ORCHESTRATION LAYER              ‚îÇ
       ‚îÇ         Apache Airflow                ‚îÇ
       ‚îÇ    (Containerized with Docker)        ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ                   ‚îÇ
                  ‚ñº                   ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ   RAW DATA LAYER ‚îÇ  ‚îÇ  EXTRACTION      ‚îÇ
       ‚îÇ      AWS S3      ‚îÇ  ‚îÇ  & VALIDATION    ‚îÇ
       ‚îÇ    (Parquet)     ‚îÇ  ‚îÇ  - Data Quality  ‚îÇ
       ‚îÇ                  ‚îÇ  ‚îÇ  - Metadata      ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ       DATA WAREHOUSE                  ‚îÇ
       ‚îÇ         Snowflake                     ‚îÇ
       ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
       ‚îÇ   ‚îÇ      RAW SCHEMA              ‚îÇ   ‚îÇ
       ‚îÇ   ‚îÇ  - agents                    ‚îÇ   ‚îÇ
       ‚îÇ   ‚îÇ  - customers                 ‚îÇ   ‚îÇ
       ‚îÇ   ‚îÇ  - call_logs                 ‚îÇ   ‚îÇ
       ‚îÇ   ‚îÇ  - social_media              ‚îÇ   ‚îÇ
       ‚îÇ   ‚îÇ  - web_forms                 ‚îÇ   ‚îÇ
       ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ      TRANSFORMATION LAYER             ‚îÇ
       ‚îÇ           dbt Core                    ‚îÇ
       ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
       ‚îÇ   ‚îÇ   STAGING MODELS             ‚îÇ   ‚îÇ
       ‚îÇ   ‚îÇ   - stg_agents               ‚îÇ   ‚îÇ
       ‚îÇ   ‚îÇ   - stg_customers            ‚îÇ   ‚îÇ
       ‚îÇ   ‚îÇ   - stg_call_logs            ‚îÇ   ‚îÇ
       ‚îÇ   ‚îÇ   - stg_social_media         ‚îÇ   ‚îÇ
       ‚îÇ   ‚îÇ   - stg_web_forms            ‚îÇ   ‚îÇ
       ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
       ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
       ‚îÇ   ‚îÇ   INTERMEDIATE MODELS        ‚îÇ   ‚îÇ
       ‚îÇ   ‚îÇ   - int_complaints_unified   ‚îÇ   ‚îÇ
       ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
       ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
       ‚îÇ   ‚îÇ   MART MODELS                ‚îÇ   ‚îÇ
       ‚îÇ   ‚îÇ   - fct_complaints           ‚îÇ   ‚îÇ
       ‚îÇ   ‚îÇ   - dim_customers            ‚îÇ   ‚îÇ
       ‚îÇ   ‚îÇ   - dim_agents               ‚îÇ   ‚îÇ
       ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ       CONSUMPTION LAYER               ‚îÇ
       ‚îÇ   - BI Dashboards (Power BI/Tableau) ‚îÇ
       ‚îÇ   - ML Models                         ‚îÇ
       ‚îÇ   - Analytics APIs                    ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Technology Stack

### Core Technologies
| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Orchestration** | Apache Airflow 3.0.6 | Workflow management and scheduling |
| **Data Warehouse** | Snowflake | Centralized data storage |
| **Transformation** | dbt Core 1.8.0 | Data modeling and transformation |
| **Object Storage** | AWS S3 | Raw data lake storage |
| **Containerization** | Docker & Docker Compose | Environment consistency |
| **CI/CD** | GitHub Actions | Automated testing and deployment |
| **IaC** | Terraform | Infrastructure provisioning |
| **Programming** | Python 3.12 | ETL logic and data processing |

### Key Python Libraries
- `apache-airflow-providers-amazon` - AWS integration
- `apache-airflow-providers-snowflake` - Snowflake connector
- `boto3` - AWS SDK
- `pandas` - Data manipulation
- `pyarrow` - Parquet file handling
- `gspread` - Google Sheets API
- `psycopg2-binary` - PostgreSQL connector

---

## üìÅ Project Structure

```
coretelecoms-data-platform/
‚îÇ
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îú‚îÄ‚îÄ ci.yml                 # Continuous Integration
‚îÇ       ‚îî‚îÄ‚îÄ cd.yml                 # Continuous Deployment
‚îÇ
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îú‚îÄ‚îÄ customer_complaints_pipeline.py  # Main Airflow DAG
‚îÇ   ‚îî‚îÄ‚îÄ service_account_key.json        # Google Sheets credentials
‚îÇ
‚îú‚îÄ‚îÄ include/
‚îÇ   ‚îî‚îÄ‚îÄ scripts/
‚îÇ       ‚îú‚îÄ‚îÄ extract/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ s3_extractor.py        # S3 data extraction
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ google_sheets_extractor.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ postgres_extractor.py
‚îÇ       ‚îú‚îÄ‚îÄ load/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ s3_loader.py           # Load to S3
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ warehouse_loader.py    # Snowflake DDL
‚îÇ       ‚îî‚îÄ‚îÄ utils/
‚îÇ           ‚îú‚îÄ‚îÄ logger.py              # Logging configuration
‚îÇ           ‚îú‚îÄ‚îÄ config.py              # Configuration management
‚îÇ           ‚îú‚îÄ‚îÄ aws_utils.py           # AWS utilities
‚îÇ           ‚îî‚îÄ‚îÄ data_quality.py        # Data validation
‚îÇ
‚îú‚îÄ‚îÄ dbt/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ staging/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stg_agents.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stg_customers.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stg_call_logs.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stg_social_media.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stg_web_forms.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ intermediate/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ int_complaints_unified.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ marts/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ fct_complaints.sql
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ dim_customers.sql
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ dim_agents.sql
‚îÇ   ‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_quality_tests.yml
‚îÇ   ‚îú‚îÄ‚îÄ dbt_project.yml
‚îÇ   ‚îî‚îÄ‚îÄ profiles.yml
‚îÇ
‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îú‚îÄ‚îÄ main.tf                    # Main Terraform configuration
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf               # Variable definitions
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf                 # Output values
‚îÇ   ‚îú‚îÄ‚îÄ providers.tf               # Provider configurations
‚îÇ   ‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ s3/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ iam/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ networking/
‚îÇ   ‚îî‚îÄ‚îÄ backend.tf                 # Remote state configuration
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ customers/                 # Local customer data
‚îÇ   ‚îú‚îÄ‚îÄ call_logs/                 # Local call log data
‚îÇ   ‚îú‚îÄ‚îÄ social_media/              # Local social media data
‚îÇ   ‚îî‚îÄ‚îÄ agents/                    # Local agent data
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ airflow.cfg               # Airflow configuration
‚îÇ
‚îú‚îÄ‚îÄ logs/                         # Airflow logs
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml            # Docker services configuration
‚îú‚îÄ‚îÄ Dockerfile                    # Airflow image definition
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ .env.example                  # Environment variables template
‚îú‚îÄ‚îÄ .gitignore                    # Git ignore rules
‚îî‚îÄ‚îÄ README.md                     # This file
```

---

## üìä Data Sources

### 1. **Customers** 
- **Format**: CSV
- **Location**: AWS S3 (`s3://coretelecoms-source-data/customers/`)
- **Frequency**: Static
- **Key Fields**: `customer_id`, `name`, `gender`, `date_of_birth`, `email`, `address`

### 2. **Agents**
- **Format**: Google Sheets
- **Location**: Private Google Spreadsheet
- **Frequency**: Static
- **Key Fields**: `agent_id`, `name`, `experience`, `state`

### 3. **Call Center Logs**
- **Format**: CSV
- **Location**: AWS S3 (`s3://coretelecoms-source-data/call_logs/`)
- **Frequency**: Daily
- **Key Fields**: `call_id`, `customer_id`, `agent_id`, `complaint_category`, `resolution_status`, `call_start_time`, `call_end_time`

### 4. **Social Media Complaints**
- **Format**: JSON
- **Location**: AWS S3 (`s3://coretelecoms-source-data/social_media/`)
- **Frequency**: Daily
- **Key Fields**: `complaint_id`, `customer_id`, `agent_id`, `platform`, `complaint_category`, `resolution_status`

### 5. **Website Forms**
- **Format**: PostgreSQL Tables
- **Location**: AWS RDS PostgreSQL
- **Frequency**: Daily (New table per day: `web_form_request_YYYY_MM_DD`)
- **Key Fields**: `form_id`, `customer_id`, `agent_id`, `complaint_type`, `resolution_status`, `submission_date`

---

## üîÑ Pipeline Architecture

### Data Flow Stages

#### Stage 1: Extraction
```python
# Parallel extraction from all sources
extract_agent()          # Google Sheets ‚Üí DataFrame
extract_customer()       # Local CSV ‚Üí File List
extract_call_logs()      # Local CSV ‚Üí File List
extract_social_media()   # Local JSON ‚Üí File List
extract_web_forms()      # PostgreSQL ‚Üí DataFrame List
```

#### Stage 2: Data Lake Ingestion
```python
# Load to S3 Raw Layer (Parquet format)
load_agent()           # ‚Üí s3://bucket/agents/agents.parquet
load_customer()        # ‚Üí s3://bucket/customers/data.parquet
load_call_logs()       # ‚Üí s3://bucket/call_logs/year=YYYY/month=MM/day=DD/
load_social_media()    # ‚Üí s3://bucket/social_media/year=YYYY/month=MM/day=DD/
```

#### Stage 3: Data Warehouse Loading
```sql
-- COPY INTO from S3 to Snowflake
COPY INTO CORETELECOMS.RAW.agents FROM s3://...
COPY INTO CORETELECOMS.RAW.customers FROM s3://...
COPY INTO CORETELECOMS.RAW.call_logs FROM s3://...
COPY INTO CORETELECOMS.RAW.social_media FROM s3://...
```

#### Stage 4: Transformation (dbt)
```bash
dbt run    # Execute all transformations
dbt test   # Run data quality tests
```

### DAG Structure
```
start
  ‚îî‚îÄ‚îÄ> setup_database
         ‚îî‚îÄ‚îÄ> [create_agents_table, create_customers_table, 
               create_call_logs_table, create_social_media_table]
                ‚îî‚îÄ‚îÄ> tables_created
                       ‚îî‚îÄ‚îÄ> [extract_agent, extract_customer,
                             extract_call_logs, extract_social_media]
                              ‚îî‚îÄ‚îÄ> [load_agent, load_customer,
                                    load_call_logs, load_social_media]
                                     ‚îî‚îÄ‚îÄ> [load_agents_to_snowflake,
                                           load_customers_to_snowflake,
                                           load_call_logs_to_snowflake,
                                           load_social_media_to_snowflake]
                                            ‚îî‚îÄ‚îÄ> dbt_run
                                                   ‚îî‚îÄ‚îÄ> dbt_test
                                                          ‚îî‚îÄ‚îÄ> end
```

---

## üöÄ Setup Instructions

### Prerequisites
- Docker Desktop installed
- AWS Account with access keys
- Snowflake account
- Google Cloud Service Account (for Sheets API)
- GitHub account
- Terraform CLI installed

### Step 1: Clone Repository
```bash
git clone https://github.com/iamthedarksaint/CoreTelecoms.git
cd CoreTelecoms
```

### Step 2: Environment Configuration
```bash
# Copy environment template
cp .env.example .env

# Edit .env with your credentials
nano .env
```

**Required Environment Variables:**
```bash
# AWS Credentials
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_REGION=eu-north-1

# Snowflake Credentials
SNOWFLAKE_ACCOUNT=your_account.region.aws
SNOWFLAKE_USER=your_username
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_ROLE=ACCOUNTADMIN
SNOWFLAKE_DATABASE=CORETELECOMS
SNOWFLAKE_WAREHOUSE=CORETELECOM
SNOWFLAKE_SCHEMA=RAW

# Airflow
AIRFLOW_UID=50000
_AIRFLOW_WWW_USER_USERNAME=airflow
_AIRFLOW_WWW_USER_PASSWORD=airflow
```

### Step 3: Google Sheets Setup
```bash
1. Create a service account in Google Cloud Console
2. Enable Google Sheets API
3. Download service account JSON key
4. Place it as: dags/service_account_key.json
5. Share your Google Sheet with the service account email
```

### Step 4: Infrastructure Provisioning (Terraform)
```bash
cd terraform

# Initialize Terraform
terraform init

# Review the plan
terraform plan

# Apply infrastructure
terraform apply

cd ..
```

### Step 5: Build and Start Docker Containers
```bash
# Build the custom Airflow image
docker-compose build --no-cache

# Start all services
docker-compose up -d

# Check container status
docker-compose ps

# View logs
docker-compose logs -f airflow-apiserver
```

### Step 6: Configure Airflow Connections
```bash
# Access Airflow UI at http://localhost:8080
# Login: airflow / airflow

# Add Connections via UI (Admin ‚Üí Connections):

# 1. aws_source
Type: Amazon Web Services
AWS Access Key ID: your_key
AWS Secret Access Key: your_secret
Region: eu-north-1

# 2. aws_dest
Type: Amazon Web Services
AWS Access Key ID: your_key
AWS Secret Access Key: your_secret
Region: eu-north-1

# 3. snowflake_default
Type: Snowflake
Account: your_account
Login: your_username
Password: your_password
Schema: RAW
Database: CORETELECOMS
Warehouse: CORETELECOM
Role: ACCOUNTADMIN
```

### Step 7: Verify dbt Setup
```bash
# Enter Airflow worker container
docker-compose exec airflow-worker bash

# Check dbt installation
dbt --version

# Test dbt connection
cd /opt/airflow/dbt
dbt debug

exit
```

---

## ‚ñ∂Ô∏è Running the Pipeline

### Manual Trigger
1. Navigate to Airflow UI: `http://localhost:8080`
2. Find DAG: `customer_complaints_pipeline`
3. Toggle DAG to **ON**
4. Click **Play button** ‚Üí Trigger DAG

### Scheduled Execution
The pipeline runs **daily at midnight** automatically (configured with `schedule="@daily"`).

### Monitor Execution
```bash
# View real-time logs
docker-compose logs -f airflow-scheduler

# Check task status in Airflow UI
# Navigate to: DAGs ‚Üí customer_complaints_pipeline ‚Üí Grid View
```

### Re-run Failed Tasks
1. Click on the failed task box
2. Select **Clear**
3. Task will automatically retry

---

## ‚úÖ Data Quality & Testing

### Extraction Layer Validation
- **Row count checks**: Ensures data is extracted
- **Required column validation**: Verifies expected schema
- **Data type validation**: Confirms correct types
- **Null checks**: Identifies missing critical values

### dbt Data Tests
```yaml
# tests/data_quality_tests.yml
models:
  - name: fct_complaints
    tests:
      - dbt_utils.unique_combination_of_columns:
          combination_of_columns:
            - complaint_id
            - source_system
    columns:
      - name: customer_id
        tests:
          - not_null
          - relationships:
              to: ref('dim_customers')
              field: customer_id
      
      - name: agent_id
        tests:
          - not_null
          - relationships:
              to: ref('dim_agents')
              field: agent_id
      
      - name: resolution_status
        tests:
          - accepted_values:
              values: ['Resolved', 'Pending', 'Escalated']
```

### Data Quality Metrics
- **Completeness**: % of non-null values
- **Uniqueness**: Duplicate detection
- **Consistency**: Cross-source validation
- **Timeliness**: Data freshness checks
- **Accuracy**: Business rule validation

---

## üìà Monitoring & Alerts

### Airflow Monitoring
- **Web UI**: Task status, logs, and metrics at `http://localhost:8080`
- **Health checks**: Container-level health monitoring
- **Flower**: Celery worker monitoring at `http://localhost:5555` (optional)

### Alert Configuration
```python
# In DAG definition
default_args = {
    'owner': 'airflow',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': True,
    'email_on_retry': False,
    'email': ['data-team@coretelecoms.com']
}
```

### Logging
- **Centralized logs**: `/opt/airflow/logs/`
- **Task-level logging**: Available per task execution
- **Custom logger**: Structured logging with timestamps

---

## üîÑ CI/CD Pipeline

### GitHub Actions Workflows

#### Continuous Integration (`.github/workflows/ci.yml`)
[View the main configuration file](.github/workflows/sample.yaml)

### GitHub Secrets Required
```
DOCKER_USERNAME          # Docker Hub username
DOCKER_PASSWORD          # Docker Hub password
AWS_ACCESS_KEY_ID        # AWS access key
AWS_SECRET_ACCESS_KEY    # AWS secret key
SNOWFLAKE_ACCOUNT        # Snowflake account
SNOWFLAKE_PASSWORD       # Snowflake password
```

---

## üèóÔ∏è Infrastructure as Code

### Terraform Modules
[View the main configuration file](infra/main.tf))

### Terraform Commands
```bash
# Initialize
terraform init

# Plan changes
terraform plan -var-file="environments/dev.tfvars"

# Apply infrastructure
terraform apply -var-file="environments/dev.tfvars"

# Destroy infrastructure
terraform destroy -var-file="environments/dev.tfvars"
```

---

## ‚≠ê Key Features

### 1. **Production-Grade Architecture**
- Containerized services for consistency
- Separate compute, storage, and orchestration layers
- Scalable to handle millions of records

### 2. **Data Quality First**
- Built-in validation at every stage
- Automated dbt tests
- Metadata tracking for auditability

### 3. **Idempotent Pipeline**
- Safe to re-run without duplication
- Incremental loading for daily data
- Full refresh for static datasets

### 4. **Comprehensive Monitoring**
- Real-time task status in Airflow UI
- Email alerts on failures
- Detailed logging at each step

### 5. **Developer-Friendly**
- Clear code organization
- Extensive documentation
- Easy local development with Docker

### 6. **Cost-Optimized**
- Parquet format reduces storage by 80%
- Incremental processing minimizes compute
- S3 lifecycle policies for archival

---

## üîÆ Future Enhancements

### Short-Term (1-3 months)
- [ ] Real-time streaming ingestion with Kafka
- [ ] Advanced ML models for churn prediction
- [ ] Power BI/Tableau dashboards
- [ ] Slack/Email notifications for SLA breaches
- [ ] Data lineage tracking with OpenLineage

### Medium-Term (3-6 months)
- [ ] Multi-region replication
- [ ] Data masking for PII compliance
- [ ] Advanced analytics with Spark
- [ ] A/B testing framework
- [ ] Self-service data portal

### Long-Term (6+ months)
- [ ] Real-time recommendation engine
- [ ] Predictive maintenance alerts
- [ ] Customer sentiment analysis
- [ ] Integration with CRM systems
- [ ] Multi-cloud support (AWS + Azure + GCP)

---

## üìö Documentation

### Additional Resources
- [Airflow Documentation](https://airflow.apache.org/docs/)
- [dbt Documentation](https://docs.getdbt.com/)
- [Snowflake Documentation](https://docs.snowflake.com/)
- [Terraform AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)

### Project Documentation
- [Architecture Decision Records (ADRs)](./docs/architecture/)
- [Data Dictionary](./docs/data_dictionary.md)
- [API Documentation](./docs/api.md)
- [Troubleshooting Guide](./docs/troubleshooting.md)

---

## üë• Contributors

- **Your Name** - Data Engineer - [@iamthedarksaint](https://github.com/iamthedarksaint)

---

## üôè Acknowledgments

- CoreTelecoms for the business case
- Apache Airflow community
- dbt Labs for the transformation framework
- Snowflake for the data warehouse platform

---

## üìû Contact & Support

For questions, issues, or contributions:
- **Email**: bojzino128@gmail.com

---

**Built with ‚ù§Ô∏è for CoreTelecoms**